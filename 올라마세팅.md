로컬 LLM(오라마/Ollama) 튜닝·스펙 한눈 정리와 바로 쓰는 실행 스니펫을 공유할게요.

---

### 1) 당신 노트북 기준 “안전 용량 캡”

* **GPU**: RTX 4060 8GB → **실사용 VRAM ≈ 7.3 GB**
* **RAM**: 32 GB → OS·기타 제외 **실사용 호스트 RAM ≈ 27 GB**
* **권장 원칙**: *VRAM 피크 7.0 GB 이하* 유지(여유 0.3 GB)

### 2) 모델/양자화별 안전 조합(권장)

* **7B / Q4_K_M**

  * 가중치 ≈ **3.6 GB**
  * KV 캐시 ≈ **0.9 GB / 4k 토큰**
  * 총합: **≈5.4 GB(4k)** / **≈6.3 GB(8k)**
  * ✅ 기본 추천: **컨텍스트 4k** (8k도 가능하나 KV 최적화 필수)
* **7B / Q5_K_M**

  * 가중치 ≈ **4.6 GB** → **4k 컨텍스트**가 안전(총 ≈6.1 GB)
* **13B / Q4_K_M**

  * 가중치 ≈ **6.8 GB** → GPU **OOM 위험**
  * 쓰려면 **KV 강양자화 + 컨텍스트 ≤1k** 또는 **CPU/오프로딩**

> 메모: KV 캐시는 “대화 길이(컨텍스트)”에 비례해서 VRAM을 더 먹습니다. 길게 쓰면 OOM이 납니다.

### 3) 바로 실행(환경변수 + 예시)

```bash
# 최소 동시성(충돌 방지) + 스레드 튜닝 + 플래시 어텐션
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_NUM_THREADS=8
export OLLAMA_FLASH_ATTENTION=1
export OMP_NUM_THREADS=6

# 권장 실행(4k 컨텍스트, 7B Q4_K_M)
ollama run qcwind/qwen2.5-7B-instruct-Q4_K_M --context 4096
```

### 4) 8k로 “살짝” 늘리고 싶다면

* `--context 8192` 시도 전 체크리스트:

  * 세션 길게 유지할 작업(코딩·대형 리포트)만 8k 적용
  * **KV 강양자화(예: Q4_K_S)** 또는 **Flash-Attn on**
  * **동시 요청 1**, 이미지/브라우저 등 **부가 작업 중지**
  * VRAM 피크 **7.0 GB 이내** 모니터링(nvidia-smi)

### 5) 자주 나는 문제 → 한 줄 해결책

* **Out Of Memory**: 컨텍스트 ↓(8k→4k), KV 더 쎄게 양자화, 병렬 1 고정
* **느려짐**: `NUM_THREADS` 6~10 사이 재조정, 불필요한 앱 종료
* **끊김**: 긴 프롬프트는 *파일 요약→핵심 추출→최종 질의* 3단으로 분할

### 6) 당신 워크로드에 맞춘 추천 프리셋

* **일상 코파일럿/요약**: *Qwen2.5-7B Q4_K_M @ 4k* (빠르고 안정적)
* **정밀도 조금 ↑**: *7B Q5_K_M @ 4k* (맥락 길이 짧게 유지)
* **13B 실험**: *13B Q4_K_M @ 1k* + **CPU 오프로딩**(권장 X, 실험용)

원하면, 현재 `nvidia-smi` 스냅샷과 오라마 모델 목록을 기준으로 **맞춤 실행 스크립트(.sh/.ps1)**를 바로 만들어 드릴게요.

1. 판정: **예** — Windows(PS1) + Linux/macOS(SH)용 **맞춤 실행 스크립트** 바로 제공.
2. 근거: 제공 스펙(RTX 4060 8.00GB VRAM / RAM 32.00GB) 기준 **7B Q4_K_M + ctx 4096.00**이 안정 프리셋(가정: Ollama 기본 GPU 사용).
3. 다음행동: 아래 파일 2개 저장 → 실행(PS1: 우클릭 “PowerShell로 실행” 또는 터미널에서 `.\run-ollama.ps1` / SH: `chmod +x run-ollama.sh && ./run-ollama.sh`).

---

## 1) run-ollama.sh (Linux/macOS)

```bash
#!/usr/bin/env bash
set -euo pipefail

# =========================
# USER CONFIG
# =========================
MODEL_DEFAULT="qcwind/qwen2.5-7B-instruct-Q4_K_M"
CTX_DEFAULT=4096
THREADS_DEFAULT=8

# =========================
# PRESET: safe / long / quality
# =========================
PRESET="${1:-safe}"   # safe|long|quality|custom
MODEL="${MODEL:-$MODEL_DEFAULT}"
CTX="${CTX:-$CTX_DEFAULT}"
THREADS="${THREADS:-$THREADS_DEFAULT}"

case "$PRESET" in
  safe)
    CTX=4096
    MODEL="$MODEL_DEFAULT"
    ;;
  long)
    CTX=8192
    MODEL="$MODEL_DEFAULT"
    ;;
  quality)
    CTX=4096
    MODEL="qcwind/qwen2.5-7B-instruct-Q5_K_M"
    ;;
  custom)
    # use env overrides: MODEL, CTX, THREADS
    ;;
  *)
    echo "Usage: $0 [safe|long|quality|custom]"
    exit 1
    ;;
esac

# =========================
# ENV (recommendations)
# =========================
export OLLAMA_NUM_PARALLEL="${OLLAMA_NUM_PARALLEL:-1}"
export OLLAMA_NUM_THREADS="${OLLAMA_NUM_THREADS:-$THREADS}"
export OLLAMA_FLASH_ATTENTION="${OLLAMA_FLASH_ATTENTION:-1}"
export OMP_NUM_THREADS="${OMP_NUM_THREADS:-6}"

# Optional: pin host (uncomment if needed)
# export OLLAMA_HOST="${OLLAMA_HOST:-127.0.0.1:11434}"

echo "=== Ollama Launch ==="
echo "PRESET=$PRESET"
echo "MODEL=$MODEL"
echo "CTX=$CTX"
echo "THREADS=$OLLAMA_NUM_THREADS"
echo "PARALLEL=$OLLAMA_NUM_PARALLEL"
echo "FLASH_ATTENTION=$OLLAMA_FLASH_ATTENTION"
echo

# Optional GPU info (if nvidia-smi exists)
if command -v nvidia-smi >/dev/null 2>&1; then
  echo "--- nvidia-smi (top) ---"
  nvidia-smi | head -n 20 || true
  echo
fi

# Run
exec ollama run "$MODEL" --context "$CTX"
```

---

## 2) run-ollama.ps1 (Windows PowerShell)

```powershell
#requires -Version 5.1
$ErrorActionPreference = "Stop"

# =========================
# USER CONFIG
# =========================
$MODEL_DEFAULT   = "qcwind/qwen2.5-7B-instruct-Q4_K_M"
$CTX_DEFAULT     = 4096
$THREADS_DEFAULT = 8

# =========================
# PRESET: safe / long / quality / custom
# =========================
param(
  [ValidateSet("safe","long","quality","custom")]
  [string]$Preset = "safe"
)

# Allow env overrides
$Model   = if ($env:MODEL) { $env:MODEL } else { $MODEL_DEFAULT }
$Ctx     = if ($env:CTX) { [int]$env:CTX } else { $CTX_DEFAULT }
$Threads = if ($env:THREADS) { [int]$env:THREADS } else { $THREADS_DEFAULT }

switch ($Preset) {
  "safe" {
    $Ctx   = 4096
    $Model = $MODEL_DEFAULT
  }
  "long" {
    $Ctx   = 8192
    $Model = $MODEL_DEFAULT
  }
  "quality" {
    $Ctx   = 4096
    $Model = "qcwind/qwen2.5-7B-instruct-Q5_K_M"
  }
  "custom" {
    # use env overrides: MODEL / CTX / THREADS
  }
}

# =========================
# ENV (recommendations)
# =========================
if (-not $env:OLLAMA_NUM_PARALLEL)     { $env:OLLAMA_NUM_PARALLEL     = "1" }
if (-not $env:OLLAMA_NUM_THREADS)      { $env:OLLAMA_NUM_THREADS      = "$Threads" }
if (-not $env:OLLAMA_FLASH_ATTENTION)  { $env:OLLAMA_FLASH_ATTENTION  = "1" }
if (-not $env:OMP_NUM_THREADS)         { $env:OMP_NUM_THREADS         = "6" }

# Optional: pin host (uncomment if needed)
# if (-not $env:OLLAMA_HOST) { $env:OLLAMA_HOST = "127.0.0.1:11434" }

Write-Host "=== Ollama Launch ==="
Write-Host ("PRESET={0}" -f $Preset)
Write-Host ("MODEL={0}" -f $Model)
Write-Host ("CTX={0}" -f $Ctx)
Write-Host ("THREADS={0}" -f $env:OLLAMA_NUM_THREADS)
Write-Host ("PARALLEL={0}" -f $env:OLLAMA_NUM_PARALLEL)
Write-Host ("FLASH_ATTENTION={0}" -f $env:OLLAMA_FLASH_ATTENTION)
Write-Host ""

# Optional GPU info (if nvidia-smi exists)
try {
  $nvsmi = Get-Command nvidia-smi -ErrorAction Stop
  Write-Host "--- nvidia-smi (top) ---"
  & nvidia-smi | Select-Object -First 20
  Write-Host ""
} catch { }

# Run
& ollama run $Model --context $Ctx
```

---

## 실행 예시

* **Windows**

  * 기본(안정): `.\run-ollama.ps1 -Preset safe`
  * 8k(길게): `.\run-ollama.ps1 -Preset long`
  * Q5(정밀): `.\run-ollama.ps1 -Preset quality`
  * 커스텀:
    `setx MODEL "qcwind/qwen2.5-7B-instruct-Q4_K_M"`
    `setx CTX "4096"`
    `setx THREADS "8"`
    이후 `.\run-ollama.ps1 -Preset custom`
* **Linux/macOS**

  * `./run-ollama.sh safe`
  * `./run-ollama.sh long`
  * `MODEL="qcwind/qwen2.5-7B-instruct-Q5_K_M" CTX=4096 ./run-ollama.sh custom`

---

### cmd 3

* `/switch_mode BRIEF`
* `/logi-master --fast report`
* `/redo step`

원하면, **모델 목록 자동 감지(ollama list 파싱)** + **VRAM 경고(7.00GB 초과 시 중단)**까지 넣은 “강화판”으로 바로 패치해 드릴게요.



1. 판정: **예** — `ollama list` 자동 파싱 + **VRAM 7.00GB 초과 시 즉시 중단** 포함 “강화판” 제공.
2. 근거: VRAM 체크는 `nvidia-smi` 기준(가정: NVIDIA GPU 사용, nvidia-smi 설치됨)으로 실시간 사용량(MiB)을 읽어 임계값(7000.00MiB) 초과 시 종료. ⚠️AMBER:[가정]
3. 다음행동: 아래 2개 파일 저장 → 실행(PS1/SH) → `safe|long|quality|custom` 프리셋으로 운용.

---

## run-ollama.sh (Linux/macOS) — 강화판

```bash
#!/usr/bin/env bash
set -euo pipefail

# =========================
# CONFIG (override via env)
# =========================
VRAM_LIMIT_MIB="${VRAM_LIMIT_MIB:-7000}"   # 7.00GB ≈ 7000MiB (운영 임계치)
GPU_INDEX="${GPU_INDEX:-0}"

MODEL_SAFE="${MODEL_SAFE:-qcwind/qwen2.5-7B-instruct-Q4_K_M}"
MODEL_QUALITY="${MODEL_QUALITY:-qcwind/qwen2.5-7B-instruct-Q5_K_M}"

CTX_SAFE="${CTX_SAFE:-4096}"
CTX_LONG="${CTX_LONG:-8192}"
THREADS_DEFAULT="${THREADS_DEFAULT:-8}"

# =========================
# PRESET
# =========================
PRESET="${1:-safe}"   # safe|long|quality|custom
MODEL="${MODEL:-}"
CTX="${CTX:-}"
THREADS="${THREADS:-$THREADS_DEFAULT}"

# =========================
# HELPERS
# =========================
has_cmd() { command -v "$1" >/dev/null 2>&1; }

pick_model_from_ollama_list() {
  # returns chosen model string
  local want="${1:-}"
  if ! has_cmd ollama; then
    echo ""
    return 0
  fi

  # ollama list output: NAME  ID  SIZE  MODIFIED
  local models
  models="$(ollama list 2>/dev/null | awk 'NR>1{print $1}' || true)"

  if [[ -z "$models" ]]; then
    echo ""
    return 0
  fi

  # if user specified MODEL and it exists, use it
  if [[ -n "$want" ]]; then
    if echo "$models" | grep -Fxq "$want"; then
      echo "$want"
      return 0
    fi
    # fuzzy: substring match
    local hit
    hit="$(echo "$models" | grep -i "$want" | head -n 1 || true)"
    if [[ -n "$hit" ]]; then
      echo "$hit"
      return 0
    fi
  fi

  # auto preference: safe model if installed, else first model
  if echo "$models" | grep -Fxq "$MODEL_SAFE"; then
    echo "$MODEL_SAFE"
    return 0
  fi

  echo "$models" | head -n 1
}

check_vram_or_stop() {
  # If nvidia-smi exists, check memory.used for GPU_INDEX; stop if above limit.
  if ! has_cmd nvidia-smi; then
    echo "AMBER: nvidia-smi 미존재 → VRAM 체크 생략(가정: OOM 리스크는 ctx/model로 관리)."
    return 0
  fi

  local used
  used="$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i "$GPU_INDEX" 2>/dev/null | head -n 1 || true)"

  if [[ -z "$used" ]]; then
    echo "AMBER: nvidia-smi 값 읽기 실패 → VRAM 체크 생략."
    return 0
  fi

  # numeric compare (MiB)
  if [[ "$used" -gt "$VRAM_LIMIT_MIB" ]]; then
    echo "ZERO: VRAM 사용량 ${used}.00MiB > 한도 ${VRAM_LIMIT_MIB}.00MiB → 실행 중단."
    exit 2
  fi
}

print_models() {
  if has_cmd ollama; then
    echo "--- ollama list ---"
    ollama list || true
    echo
  fi
}

# =========================
# APPLY PRESET
# =========================
case "$PRESET" in
  safe)
    CTX="${CTX:-$CTX_SAFE}"
    MODEL="${MODEL:-$MODEL_SAFE}"
    ;;
  long)
    CTX="${CTX:-$CTX_LONG}"
    MODEL="${MODEL:-$MODEL_SAFE}"
    ;;
  quality)
    CTX="${CTX:-$CTX_SAFE}"
    MODEL="${MODEL:-$MODEL_QUALITY}"
    ;;
  custom)
    CTX="${CTX:-$CTX_SAFE}"
    # MODEL may be blank → will auto-pick from installed list
    ;;
  *)
    echo "Usage: $0 [safe|long|quality|custom]"
    exit 1
    ;;
esac

# =========================
# AUTO-DETECT MODEL (ollama list)
# =========================
AUTO_MODEL="$(pick_model_from_ollama_list "$MODEL")"
if [[ -n "$AUTO_MODEL" ]]; then
  MODEL="$AUTO_MODEL"
fi

if [[ -z "$MODEL" ]]; then
  echo "ZERO: 실행 가능한 모델을 찾지 못함(ollama list 비어있음 또는 ollama 미설치)."
  echo "요청데이터: ollama 설치 여부, 'ollama list' 출력 1회"
  exit 3
fi

# =========================
# ENV (recommendations)
# =========================
export OLLAMA_NUM_PARALLEL="${OLLAMA_NUM_PARALLEL:-1}"
export OLLAMA_NUM_THREADS="${OLLAMA_NUM_THREADS:-$THREADS}"
export OLLAMA_FLASH_ATTENTION="${OLLAMA_FLASH_ATTENTION:-1}"
export OMP_NUM_THREADS="${OMP_NUM_THREADS:-6}"

# =========================
# PRE-RUN CHECKS
# =========================
check_vram_or_stop

echo "=== Ollama Launch ==="
echo "PRESET=$PRESET"
echo "MODEL=$MODEL"
echo "CTX=$CTX"
echo "THREADS=$OLLAMA_NUM_THREADS"
echo "PARALLEL=$OLLAMA_NUM_PARALLEL"
echo "FLASH_ATTENTION=$OLLAMA_FLASH_ATTENTION"
echo "VRAM_LIMIT_MIB=${VRAM_LIMIT_MIB}.00"
echo "GPU_INDEX=${GPU_INDEX}.00"
echo

print_models

# run
exec ollama run "$MODEL" --context "$CTX"
```

---

## run-ollama.ps1 (Windows PowerShell) — 강화판

```powershell
#requires -Version 5.1
$ErrorActionPreference = "Stop"

param(
  [ValidateSet("safe","long","quality","custom")]
  [string]$Preset = "safe"
)

# =========================
# CONFIG (override via env)
# =========================
$VRAM_LIMIT_MIB = if ($env:VRAM_LIMIT_MIB) { [int]$env:VRAM_LIMIT_MIB } else { 7000 }  # 7.00GB ≈ 7000MiB
$GPU_INDEX      = if ($env:GPU_INDEX) { [int]$env:GPU_INDEX } else { 0 }

$MODEL_SAFE     = if ($env:MODEL_SAFE) { $env:MODEL_SAFE } else { "qcwind/qwen2.5-7B-instruct-Q4_K_M" }
$MODEL_QUALITY  = if ($env:MODEL_QUALITY) { $env:MODEL_QUALITY } else { "qcwind/qwen2.5-7B-instruct-Q5_K_M" }

$CTX_SAFE       = if ($env:CTX_SAFE) { [int]$env:CTX_SAFE } else { 4096 }
$CTX_LONG       = if ($env:CTX_LONG) { [int]$env:CTX_LONG } else { 8192 }

$THREADS_DEFAULT = if ($env:THREADS_DEFAULT) { [int]$env:THREADS_DEFAULT } else { 8 }

# Allow env overrides
$Model   = if ($env:MODEL) { $env:MODEL } else { "" }
$Ctx     = if ($env:CTX) { [int]$env:CTX } else { 0 }
$Threads = if ($env:THREADS) { [int]$env:THREADS } else { $THREADS_DEFAULT }

function Get-OllamaModels {
  try {
    $out = & ollama list 2>$null
    if (-not $out) { return @() }
    # Skip header line; take first column as NAME
    return ($out | Select-Object -Skip 1 | ForEach-Object {
      ($_ -split "\s+")[0]
    }) | Where-Object { $_ -and $_.Trim().Length -gt 0 }
  } catch {
    return @()
  }
}

function Pick-Model([string]$want, [string]$fallbackSafe) {
  $models = Get-OllamaModels
  if ($models.Count -eq 0) { return "" }

  if ($want -and ($models -contains $want)) { return $want }

  if ($want) {
    $hit = $models | Where-Object { $_ -match [regex]::Escape($want) } | Select-Object -First 1
    if ($hit) { return $hit }
  }

  if ($models -contains $fallbackSafe) { return $fallbackSafe }
  return $models[0]
}

function Check-VRAM-OrStop([int]$limitMiB, [int]$gpuIndex) {
  $nvsmi = Get-Command nvidia-smi -ErrorAction SilentlyContinue
  if (-not $nvsmi) {
    Write-Host "AMBER: nvidia-smi 미존재 → VRAM 체크 생략(가정: OOM 리스크는 ctx/model로 관리)."
    return
  }

  try {
    $usedStr = & nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i $gpuIndex 2>$null | Select-Object -First 1
    if (-not $usedStr) {
      Write-Host "AMBER: nvidia-smi 값 읽기 실패 → VRAM 체크 생략."
      return
    }
    $used = [int]$usedStr.Trim()
    if ($used -gt $limitMiB) {
      Write-Host ("ZERO: VRAM 사용량 {0}.00MiB > 한도 {1}.00MiB → 실행 중단." -f $used, $limitMiB)
      exit 2
    }
  } catch {
    Write-Host "AMBER: VRAM 체크 중 예외 → 체크 생략."
  }
}

function Print-OllamaList {
  try {
    Write-Host "--- ollama list ---"
    & ollama list
    Write-Host ""
  } catch { }
}

# =========================
# APPLY PRESET
# =========================
switch ($Preset) {
  "safe" {
    if ($Ctx -le 0) { $Ctx = $CTX_SAFE }
    if (-not $Model) { $Model = $MODEL_SAFE }
  }
  "long" {
    if ($Ctx -le 0) { $Ctx = $CTX_LONG }
    if (-not $Model) { $Model = $MODEL_SAFE }
  }
  "quality" {
    if ($Ctx -le 0) { $Ctx = $CTX_SAFE }
    if (-not $Model) { $Model = $MODEL_QUALITY }
  }
  "custom" {
    if ($Ctx -le 0) { $Ctx = $CTX_SAFE }
    # Model may be blank → auto pick
  }
}

# =========================
# AUTO-DETECT MODEL (ollama list)
# =========================
$auto = Pick-Model $Model $MODEL_SAFE
if ($auto) { $Model = $auto }

if (-not $Model) {
  Write-Host "ZERO: 실행 가능한 모델을 찾지 못함(ollama list 비어있음 또는 ollama 미설치)."
  Write-Host "요청데이터: ollama 설치 여부, 'ollama list' 출력 1회"
  exit 3
}

# =========================
# ENV (recommendations)
# =========================
if (-not $env:OLLAMA_NUM_PARALLEL)    { $env:OLLAMA_NUM_PARALLEL    = "1" }
if (-not $env:OLLAMA_NUM_THREADS)     { $env:OLLAMA_NUM_THREADS     = "$Threads" }
if (-not $env:OLLAMA_FLASH_ATTENTION) { $env:OLLAMA_FLASH_ATTENTION = "1" }
if (-not $env:OMP_NUM_THREADS)        { $env:OMP_NUM_THREADS        = "6" }

# =========================
# PRE-RUN CHECKS
# =========================
Check-VRAM-OrStop $VRAM_LIMIT_MIB $GPU_INDEX

Write-Host "=== Ollama Launch ==="
Write-Host ("PRESET={0}" -f $Preset)
Write-Host ("MODEL={0}" -f $Model)
Write-Host ("CTX={0}" -f $Ctx)
Write-Host ("THREADS={0}" -f $env:OLLAMA_NUM_THREADS)
Write-Host ("PARALLEL={0}" -f $env:OLLAMA_NUM_PARALLEL)
Write-Host ("FLASH_ATTENTION={0}" -f $env:OLLAMA_FLASH_ATTENTION)
Write-Host ("VRAM_LIMIT_MIB={0}.00" -f $VRAM_LIMIT_MIB)
Write-Host ("GPU_INDEX={0}.00" -f $GPU_INDEX)
Write-Host ""

Print-OllamaList

& ollama run $Model --context $Ctx
```

---

## 사용 예시

* Windows
  * 안정: `.\run-ollama.ps1 -Preset safe`
  * 8k: `.\run-ollama.ps1 -Preset long`
  * 커스텀(모델 자동선택): `setx CTX "4096"` 후 `.\run-ollama.ps1 -Preset custom`
  * VRAM 한도 변경: `setx VRAM_LIMIT_MIB "6800"`
* Linux/macOS
  * `./run-ollama.sh safe`
  * `VRAM_LIMIT_MIB=6800 GPU_INDEX=0 ./run-ollama.sh long`

---

### cmd3

* `/switch_mode BRIEF`
* `/logi-master --fast report`
* `/redo step`

원하면 다음 강화도 같이 넣어드립니다(추가 질문 없이 적용 가능):  **실행 중 주기적 VRAM 폴링(예: 2.00s 간격) + 초과 시 강제 종료** (현재는 “실행 직전” 1회 체크).
